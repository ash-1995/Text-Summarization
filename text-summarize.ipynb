{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Auto Summarizing text</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p1>This is a technique of creating a summary for a given piece of text. This involves assigning values/weights to each word in the text based on the frequency of their occurence. Interestingly, it turns out that authors of articles/content tend to use use those words that describe the theme of the content a lot more that other words.</p1>\n",
    "\n",
    "<p2><b>This means, if we use frequency based encoding, we would be able to get those sentences that has the most valued words, which can be used as a summary/abstract</b></p2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Steps involved in Auto Summarizing text</h3>\n",
    "<ol>\n",
    "<li>Converting the text in to sentences and into words</li>\n",
    "<li>Create a set of stopwords that needs to be removed from the list of words</li>\n",
    "<li>Remove the stopwords from the list of words</li>\n",
    "<li>Create a frequency distribution using the cleaned list of words</li>    \n",
    "<li>Update the weightage of each sentence based on the words present in it</li>\n",
    "<li>S</li>    \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the text file and read its content into a variable called text\n",
    "\n",
    "with open('baba.txt') as file:\n",
    "    text=file.read()\n",
    "text=text.lower()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text into sentences and store it in a variable\n",
    "\n",
    "sentences_in_text=sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text into words\n",
    "\n",
    "words_in_text=word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a set of stop words to be removed\n",
    "\n",
    "_stopwords=set(list(punctuation)+stopwords.words('english'))\n",
    "_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing the stopwords from our words list to speed up processing and to avoid the inteference of these words\n",
    "\n",
    "cleaned_words=[a for a in words_in_text if a not in _stopwords]\n",
    "len(cleaned_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'indian': 9, 'baba': 8, 'army': 7, 'soldiers': 6, '``': 4, \"''\": 4, 'singh': 4, 'nathu': 4, 'la': 4, 'sikkim': 4, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frequency Distribution is a table that contains the list of words in a column and their \n",
    "#respective frequency of occurence in another.\n",
    "\n",
    "# We can use the inbuilt functionn called FreqDist to get this done for our cleaned list\n",
    "\n",
    "freq=FreqDist(cleaned_words)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output of the method FreqDist is a dictonary\n",
    "\n",
    "type(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indian',\n",
       " 'baba',\n",
       " 'army',\n",
       " 'soldiers',\n",
       " '``',\n",
       " \"''\",\n",
       " 'singh',\n",
       " 'nathu',\n",
       " 'la',\n",
       " 'sikkim']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the method nlargest takes 3 inputs, ie the top N values to return, the collection on which the sorting \n",
    "# has to be done, and the value based on which the sorting has to be done (ie, should we sort it based on)\n",
    "# the value of the key or the its value\n",
    "\n",
    "\n",
    "nlargest(10,freq,key=freq.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaultdict is a special type of dictonary, which doesn't throw errors if the key we are looking for is absent \n",
    "# in the dictonart\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "ranking=defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We would be looping through each sentence and increasin the weightage of each sentence based on the\n",
    "# words present in it. At the end, we will have a weightage for all the sentences and we can select \n",
    "# the top sentences based on the weightage\n",
    "\n",
    "for i,sent in enumerate(sentences_in_text):\n",
    "    for w in word_tokenize(sent.lower()):\n",
    "        if w in freq:\n",
    "            ranking[i]+=freq[w]\n",
    "\n",
    "\n",
    "# The enumerate will create a list of tuples, where each tuple has the index of the sentence as the fist\n",
    "# element and the sentence itselt as the second element.\n",
    "\n",
    "# In short, we break the sentence into words, run through each word checking if its present in the\n",
    "# cleaned list of words. If yes, we add its frequency of occurence to the weightage of this sentence.\n",
    "# And this process is done for all the sentences of the text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 52,\n",
       "             1: 52,\n",
       "             2: 26,\n",
       "             3: 66,\n",
       "             4: 14,\n",
       "             5: 7,\n",
       "             6: 19,\n",
       "             7: 85,\n",
       "             8: 60,\n",
       "             9: 17,\n",
       "             10: 5,\n",
       "             11: 7,\n",
       "             12: 16,\n",
       "             13: 57,\n",
       "             14: 21,\n",
       "             15: 27,\n",
       "             16: 12,\n",
       "             17: 21,\n",
       "             18: 40,\n",
       "             19: 34,\n",
       "             20: 29,\n",
       "             21: 29})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ranking dicronary has the indices of the sentences and their respective weightage as its value.\n",
    "# We can now sort the dict based on the values and get the top 5/10 keys, which denote the index of the\n",
    "# sentences that summarizes our text\n",
    "\n",
    "ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we go back to nlargest method to perform this sorting, which would help us get the sentence with\n",
    "# most weightage\n",
    "\n",
    "to_be_used=nlargest(5,ranking,key=ranking.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captain \"baba\" harbhajan singh (30 august 1946 – 4 october 2019)[1] was an indian army soldier.',\n",
       " 'many of his faithful - chiefly indian army personnel posted in and around the nathu la and the sino-indian border between the state of sikkim and tibet autonomous region - have come to believe his spirit protects every soldier in the inhospitable high-altitude terrain of the eastern himalayas.',\n",
       " \"harbhajan singh's early death at the age of 22 is the subject of legend and religious veneration that has become popular among indian army regulars (jawans), the people of his village and apparently soldiers of the chinese people's liberation army (pla) across the border guarding the indo-chinese border between sikkim and tibet.\",\n",
       " '[3]\\n\\nthe official version of his death is that he was a martyr of battle at the 14,500 feet (4,400 m) nathu la, a mountain pass between tibet and sikkim where many battles took place between the indian army and the pla during the 1965 sino-indian war.',\n",
       " 'many indian soldiers have revealed that in the event of a war between india and china, baba would warn the indian soldiers of an impending attack at least three days in advance.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sentences_in_text[j] for j in sorted(to_be_used)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Lets put all we have done into a method</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method takes the text content and the number of lines of summary that is required as inputs and give the\n",
    "# summary of the text as output\n",
    "\n",
    "def text_summarize(text,n):\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from string import punctuation\n",
    "    from nltk.probability import FreqDist\n",
    "    from heapq import nlargest\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    sentences_in_text=sent_tokenize(text)\n",
    "    assert n<len(sentences_in_text), \"The number of summary point should be less than the number of lines in the actual text\"\n",
    "    words_in_text=word_tokenize(text.lower())\n",
    "    \n",
    "    _stopwords=set(stopwords.words('english')+list(punctuation))\n",
    "    \n",
    "    cleaned_words=[word for word in words_in_text if word not in _stopwords]\n",
    "    \n",
    "    freq=FreqDist(cleaned_words)\n",
    "    ranking=defaultdict(int)\n",
    "    \n",
    "    for i,sent in enumerate(sentences_in_text):\n",
    "        for w in word_tokenize(sent.lower()):\n",
    "            if w in freq:\n",
    "                ranking[i]+=freq[w]\n",
    "    \n",
    "    sents_to_returned=nlargest(n,ranking,key=ranking.get)\n",
    "    return [sentences_in_text[i] for i in sorted(sents_to_returned)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Lets test this function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=text_summarize(text,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['captain \"baba\" harbhajan singh (30 august 1946 – 4 october 2019)[1] was an indian army soldier.',\n",
       " 'many of his faithful - chiefly indian army personnel posted in and around the nathu la and the sino-indian border between the state of sikkim and tibet autonomous region - have come to believe his spirit protects every soldier in the inhospitable high-altitude terrain of the eastern himalayas.',\n",
       " \"harbhajan singh's early death at the age of 22 is the subject of legend and religious veneration that has become popular among indian army regulars (jawans), the people of his village and apparently soldiers of the chinese people's liberation army (pla) across the border guarding the indo-chinese border between sikkim and tibet.\",\n",
       " '[3]\\n\\nthe official version of his death is that he was a martyr of battle at the 14,500 feet (4,400 m) nathu la, a mountain pass between tibet and sikkim where many battles took place between the indian army and the pla during the 1965 sino-indian war.',\n",
       " 'many indian soldiers have revealed that in the event of a war between india and china, baba would warn the indian soldiers of an impending attack at least three days in advance.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Thanks for your time!</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
